{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the list of pages from 1-10 in goodreads website\n",
    "def get_list_pages_of_10(url_list_10):\n",
    "    url_list = []\n",
    "    for page_number in range(0,11):\n",
    "        #remember that the end is ?page=\n",
    "        url_page = url_list_10+str(page_number)\n",
    "        url_list.append(url_page)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10pages = get_list_pages_of_10('https://www.goodreads.com/list/show/1.Best_Books_Ever?page=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get href 100 links\n",
    "def get_links_for_100(url_one_page):\n",
    "    url_page = requests.get(url_one_page)\n",
    "    soup = BeautifulSoup(url_page.content, 'html.parser')\n",
    "    href_link_list =[]\n",
    "    for link in soup.find_all('a', class_='bookTitle') :\n",
    "        if link.has_attr('href'):\n",
    "            href_link_list.append(\"https://www.goodreads.com\" + str(link.attrs['href']))\n",
    "    return href_link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_1000_links = get_all_1000_links('https://www.goodreads.com/list/show/1.Best_Books_Ever?page=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_1000_links(url_list):\n",
    "    url_10_pages = get_list_pages_of_10(url_list)\n",
    "    all_links = []\n",
    "    for link in url_10_pages:\n",
    "        all_links.append(get_links_for_100(link))\n",
    "    flat_list_of_1000 = []\n",
    "    for elem in all_links:\n",
    "        flat_list_of_1000.extend(elem)\n",
    "    return flat_list_of_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit list_of_1000_links = get_all_1000_links('https://www.goodreads.com/list/show/1.Best_Books_Ever?page=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_1000_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_of_100_links))\n",
    "print(list_of_100_links[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get title\n",
    "def get_title(soup_detail):\n",
    "    try:\n",
    "        title = soup_detail.find(\"h1\", {\"id\":\"bookTitle\"}).text.replace(\"\\n\",\"\").replace(\"      \",\"\")\n",
    "    except:\n",
    "        title = np.nan\n",
    "    return title\n",
    "\n",
    "# get author\n",
    "def get_author(soup_detail):\n",
    "    try:\n",
    "        author = soup_detail.find(\"div\", {\"class\":\"authorName__container\"}).text.replace(\"\\n\",\"\")\n",
    "    except:\n",
    "        author = np.nan\n",
    "    return author\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get ratings\n",
    "def get_rating(soup_detail):\n",
    "    try:\n",
    "        rating = soup_detail.find(\"meta\", {\"itemprop\":\"ratingCount\"}).text.replace(\"\\n\",\"\").split()\n",
    "        rating = rating[0]        \n",
    "    except:\n",
    "        rating = np.nan      \n",
    "    return rating\n",
    "\n",
    "\n",
    "# get avgratings\n",
    "book_avgratings = []\n",
    "def get_avgrating(soup_detail):\n",
    "    try:\n",
    "        avgrating = soup_detail.find(\"span\", {\"itemprop\":\"ratingValue\"}).text.replace(\"\\n\",\"\").replace(' ','')\n",
    "    except:\n",
    "        avgrating = np.nan\n",
    "    return avgrating\n",
    "\n",
    "# get pages\n",
    "book_pages = []\n",
    "def get_page(soup_detail):\n",
    "    try:\n",
    "        page = soup_detail.find(\"span\", {\"itemprop\":\"numberOfPages\"}).text.split()\n",
    "        page = page[0]\n",
    "    except:\n",
    "        page = np.nan\n",
    "    return page\n",
    "\n",
    "\n",
    "# get years\n",
    "book_publish_year = []\n",
    "def get_year(soup_detail):\n",
    "    try:\n",
    "        publish_year = soup_detail.find(\"nobr\", {\"class\":\"greyText\"}).text.replace(\"\\n\",\"\").replace(\")\",\"\").split()\n",
    "        publish_year = publish_year[-1]\n",
    "    except:\n",
    "        publish_year = np.nan\n",
    "    return publish_year\n",
    "\n",
    "# get award\n",
    "def get_award(soup_detail):\n",
    "    try:\n",
    "        award = soup_detail.find(\"div\", {\"itemprop\":\"awards\"}).text.replace(\"\\n\",\"\")\n",
    "    except:\n",
    "        award = np.nan\n",
    "    return award\n",
    "\n",
    "# get genres\n",
    "def get_genres(soup_detail):\n",
    "    try:\n",
    "        genres = soup_detail.find_all(class_=\"actionLinkLite bookPageGenreLink\")\n",
    "        genres = genres[:3]\n",
    "        genres = [genre.get_text() for genre in genres]\n",
    "    except:\n",
    "        genres = np.nan\n",
    "    return genres\n",
    "\n",
    "#get the book reviews\n",
    "def get_review(soup_detail):\n",
    "    try:\n",
    "        review = soup_detail.find(\"meta\", {\"itemprop\":\"reviewCount\"}).text.replace(\"\\n\",\"\").split()\n",
    "        review = review[0]\n",
    "    except:\n",
    "        review = np.nan\n",
    "    return review\n",
    "\n",
    "# get series\n",
    "def get_series(soup_detail):\n",
    "    try:\n",
    "        series = soup_detail.find(id=\"bookSeries\").text.strip()\n",
    "        if len(series) == 0:\n",
    "            series = False\n",
    "        else: \n",
    "            series = True\n",
    "    except:\n",
    "        series = np.nan\n",
    "    return series\n",
    "\n",
    "# get places\n",
    "def get_places(soup_detail):\n",
    "    try:\n",
    "        places = soup_detail.find(\"div\", {'id':\"bookDataBox\"}).find('span',class_=\"darkGreyText\").text.replace(\"(\",\"\").replace(\")\",\"\").strip()\n",
    "    except:\n",
    "        places = np.nan\n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(url_list):\n",
    "    title = []\n",
    "    author = []\n",
    "    num_reviews = []\n",
    "    num_ratings = []\n",
    "    avg_rating = []\n",
    "    num_pages = []\n",
    "    original_publish_year = []\n",
    "    series = []\n",
    "    genres = []\n",
    "    awards = []\n",
    "    places = []\n",
    "    \n",
    "    for link in url_list:\n",
    "        one_book= requests.get(link)\n",
    "        soup_detail = BeautifulSoup(one_book.content, 'html.parser')\n",
    "    \n",
    "\n",
    "        title.append(get_title(soup_detail))\n",
    "        author.append(get_author(soup_detail))\n",
    "        num_reviews.append(get_review(soup_detail))\n",
    "        num_ratings.append(get_rating(soup_detail))\n",
    "        avg_rating.append(get_avgrating(soup_detail))\n",
    "        num_pages.append(get_page(soup_detail))\n",
    "        original_publish_year.append(get_year(soup_detail))\n",
    "        series.append(get_series(soup_detail))\n",
    "        genres.append(get_genres(soup_detail))\n",
    "        awards.append(get_award(soup_detail))\n",
    "        places.append(get_places(soup_detail))\n",
    "        \n",
    "    data = {'url' : url_list,\n",
    "            'title':title,\n",
    "            'author':author,\n",
    "            'num_reviews':num_reviews,\n",
    "            'num_ratings': num_ratings,\n",
    "            'avg_rating':avg_rating,\n",
    "            'num_pages':num_pages,\n",
    "            'original_publish_year':original_publish_year,\n",
    "            'series':series,\n",
    "            'genres':genres,\n",
    "            'awards':awards,\n",
    "            'places':places\n",
    "           }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"df_new_1100.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### run webscrapper for 10 books!\n",
    "def get_links_for_10(url_one_page):\n",
    "    url_page = requests.get(url_one_page)\n",
    "    soup = BeautifulSoup(url_page.content, 'html.parser')\n",
    "    href_link_list =[]\n",
    "    n = 0\n",
    "    while n < 10:\n",
    "        for link in soup.find_all('a', class_='bookTitle') :\n",
    "            if link.has_attr('href'):\n",
    "                href_link_list.append(\"https://www.goodreads.com\" + str(link.attrs['href']))\n",
    "                n += 1\n",
    "    return href_link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_12576/1952549587.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\DELL\\AppData\\Local\\Temp/ipykernel_12576/1952549587.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print(len(list_of_10_links)\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#list_of_100_links = get_links_for_100('https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1')\n",
    "list_of_10_links = list_of_100_links[:10] \n",
    "print(len(list_of_10_links)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d49e6ad289614883bcb5342a4a3da3b8f2fdd690e2fb3cfaad4eebfabb9319e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('vishwa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
